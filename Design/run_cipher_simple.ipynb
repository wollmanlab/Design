{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CIPHER Design Notebook\n",
    "\n",
    "This notebook provides a simple way to run CIPHER without needing shell scripts. It includes:\n",
    "1. Data formatting\n",
    "2. Parameter setup\n",
    "3. Running CIPHER to design probes\n",
    "\n",
    "**Note**: This notebook assumes you are running on a compute node with the conda environment activated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import anndata\n",
    "\n",
    "# Add Design directory to path\n",
    "design_dir = os.path.dirname(os.path.abspath(\"__file__\")) if \"__file__\" in globals() else os.getcwd()\n",
    "sys.path.insert(0, design_dir)\n",
    "from CIPHER import CIPHER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths and Data\n",
    "\n",
    "**Update these paths to match your data location:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURE THESE PATHS =====\n",
    "# Path to your gene expression data (h5ad file or similar)\n",
    "data_path = '/path/to/your/data.h5ad'\n",
    "\n",
    "# Path to probe constraints file (CSV with gene names and max probes per gene)\n",
    "# Format: CSV with 'gene' column and 'probes' or similar column\n",
    "constraints_file = '/path/to/probe_constraints.csv'\n",
    "\n",
    "# Output directory for formatted data and results\n",
    "output_dir = './cipher_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Cell type label column name in your data\n",
    "cell_type_label = 'cell_type'  # Update to match your metadata column\n",
    "\n",
    "# ===== END CONFIGURATION =====\n",
    "\n",
    "print(f\"Data path: {data_path}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Format Data\n",
    "\n",
    "This section loads your data and formats it into the required format for CIPHER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load expression data\n",
    "print(\"Loading expression data...\")\n",
    "adata = anndata.read_h5ad(data_path)\n",
    "print(f\"Loaded data: {adata.shape[0]} cells, {adata.shape[1]} genes\")\n",
    "\n",
    "# Get gene expression matrix\n",
    "if isinstance(adata.X, np.ndarray):\n",
    "    X = torch.tensor(adata.X, dtype=torch.float32)\n",
    "else:\n",
    "    # If sparse matrix, convert to dense\n",
    "    X = torch.tensor(adata.X.todense(), dtype=torch.float32)\n",
    "\n",
    "# Normalize by library size (optional, adjust as needed)\n",
    "if 'library_size' in adata.obs.columns:\n",
    "    correction = torch.tensor(100000 / np.array(adata.obs['library_size']))\n",
    "    X = X * correction[:, None]\n",
    "\n",
    "# Get cell type labels\n",
    "if cell_type_label not in adata.obs.columns:\n",
    "    raise ValueError(f\"Cell type label '{cell_type_label}' not found in data. Available columns: {adata.obs.columns.tolist()}\")\n",
    "\n",
    "cell_types = adata.obs[cell_type_label].unique()\n",
    "categorical_converter = {k: i for i, k in enumerate(sorted(cell_types))}\n",
    "y = torch.tensor(np.array([categorical_converter[ct] for ct in adata.obs[cell_type_label]]), dtype=torch.long)\n",
    "\n",
    "print(f\"Found {len(cell_types)} cell types: {sorted(cell_types)}\")\n",
    "print(f\"Expression matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create gene constraints\n",
    "print(\"Loading gene constraints...\")\n",
    "if os.path.exists(constraints_file):\n",
    "    constraints_df = pd.read_csv(constraints_file)\n",
    "    # Try to find gene column and probe count column\n",
    "    gene_col = None\n",
    "    probe_col = None\n",
    "    for col in constraints_df.columns:\n",
    "        if 'gene' in col.lower():\n",
    "            gene_col = col\n",
    "        if 'probe' in col.lower() or 'constraint' in col.lower():\n",
    "            probe_col = col\n",
    "    \n",
    "    if gene_col is None or probe_col is None:\n",
    "        # Assume first column is gene, second is constraint\n",
    "        gene_col = constraints_df.columns[0]\n",
    "        probe_col = constraints_df.columns[1]\n",
    "    \n",
    "    # Create mapping from gene names to constraints\n",
    "    gene_to_constraint = dict(zip(constraints_df[gene_col], constraints_df[probe_col]))\n",
    "    \n",
    "    # Match constraints to genes in expression data\n",
    "    gene_names = adata.var_names.tolist()\n",
    "    constraints = np.array([gene_to_constraint.get(gene, 0) for gene in gene_names])\n",
    "    constraints = np.clip(constraints, 0, 100)  # Cap at 100 probes per gene\n",
    "else:\n",
    "    print(f\"Constraints file not found at {constraints_file}\")\n",
    "    print(\"Creating default constraints (100 probes per gene)...\")\n",
    "    constraints = np.ones(X.shape[1]) * 100\n",
    "\n",
    "print(f\"Constraints: min={constraints.min()}, max={constraints.max()}, mean={constraints.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "print(\"Splitting data into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} cells\")\n",
    "print(f\"Test set: {X_test.shape[0]} cells\")\n",
    "\n",
    "# Save formatted data\n",
    "torch.save(X_train, os.path.join(output_dir, 'X_train.pt'))\n",
    "torch.save(X_test, os.path.join(output_dir, 'X_test.pt'))\n",
    "torch.save(y_train, os.path.join(output_dir, 'y_train.pt'))\n",
    "torch.save(y_test, os.path.join(output_dir, 'y_test.pt'))\n",
    "\n",
    "# Save constraints\n",
    "constraints_df = pd.DataFrame(constraints, index=adata.var_names, columns=['constraints'])\n",
    "constraints_df.to_csv(os.path.join(output_dir, 'constraints.csv'))\n",
    "\n",
    "# Save label converter\n",
    "label_converter_df = pd.DataFrame({\n",
    "    'index': list(categorical_converter.values()),\n",
    "    'cell_type': list(categorical_converter.keys())\n",
    "})\n",
    "label_converter_df.to_csv(os.path.join(output_dir, 'categorical_converter.csv'), index=False)\n",
    "\n",
    "print(f\"\\nSaved formatted data to {output_dir}\")\n",
    "print(\"Files created:\")\n",
    "print(\"  - X_train.pt, X_test.pt\")\n",
    "print(\"  - y_train.pt, y_test.pt\")\n",
    "print(\"  - constraints.csv\")\n",
    "print(\"  - categorical_converter.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Parameters\n",
    "\n",
    "Configure CIPHER parameters. Adjust these based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameters dictionary\n",
    "parameters = {\n",
    "    # Core model parameters\n",
    "    'n_cpu': 3,\n",
    "    'n_bit': 24,  # Number of bits in encoding\n",
    "    'n_iters': 10000,  # Training iterations (increase for better results)\n",
    "    'batch_size': 500,  # Batch size (0 = use full dataset)\n",
    "    \n",
    "    # Loss function weights\n",
    "    'categorical_wt': 2.5,  # Classification accuracy weight\n",
    "    'probe_wt': 1,  # Probe count constraint weight\n",
    "    'gene_constraint_wt': 1,  # Gene constraint violation weight\n",
    "    'brightness_wt': 1,  # Target brightness weight\n",
    "    'dynamic_wt': 1,  # Dynamic range weight\n",
    "    'separation_wt': 1,  # Cell type separation weight\n",
    "    \n",
    "    # Target parameters (with _s/_e for dynamic targets)\n",
    "    'brightness_s': 4.5,  # Initial target brightness (log10 scale)\n",
    "    'brightness_e': 4.5,  # Final target brightness\n",
    "    'dynamic_fold_s': 2.0,  # Initial dynamic range fold-change\n",
    "    'dynamic_fold_e': 2.0,  # Final dynamic range fold-change\n",
    "    'separation_fold_s': 1.0,  # Initial separation fold-change\n",
    "    'separation_fold_e': 1.0,  # Final separation fold-change\n",
    "    'n_probes': 50000,  # Target total number of probes\n",
    "    \n",
    "    # Training parameters\n",
    "    'lr_s': 0.01,  # Initial learning rate\n",
    "    'lr_e': 0.01,  # Final learning rate\n",
    "    'saturation': 1.0,  # When to reach final _e values (0.0-1.0)\n",
    "    'gradient_clip': 1,  # Gradient clipping\n",
    "    'report_rt': 500,  # Report progress every N iterations\n",
    "    \n",
    "    # Model architecture\n",
    "    'decoder_n_lyr': 1,  # Number of decoder hidden layers\n",
    "    'encoder_act': 'tanh',  # Encoder activation\n",
    "    'decoder_act': 'gelu',  # Decoder activation\n",
    "    \n",
    "    # Data paths (relative to output_dir)\n",
    "    'input': output_dir,\n",
    "    'X_train': os.path.join(output_dir, 'X_train.pt'),\n",
    "    'X_test': os.path.join(output_dir, 'X_test.pt'),\n",
    "    'y_train': os.path.join(output_dir, 'y_train.pt'),\n",
    "    'y_test': os.path.join(output_dir, 'y_test.pt'),\n",
    "    'constraints': os.path.join(output_dir, 'constraints.csv'),\n",
    "    'y_label_converter_path': os.path.join(output_dir, 'categorical_converter.csv'),\n",
    "    \n",
    "    # Output directory\n",
    "    'output': os.path.join(output_dir, 'design_results'),\n",
    "    \n",
    "    # Other parameters\n",
    "    'device': 'cpu',\n",
    "    'top_n_genes': 0,  # 0 = use all genes\n",
    "    'sum_norm': 0,  # Sum normalization\n",
    "    'bit_norm': 0,  # Bit-wise normalization\n",
    "    'use_noise': 1,  # Enable noise during training\n",
    "    'continue_training': 0,  # Don't continue if model loaded\n",
    "    'best_model': 1,  # Save best model\n",
    "    'sparsity_s': 0.95,  # Target sparsity\n",
    "    'sparsity_e': 0.95,\n",
    "    'sparsity_wt': 0,  # Sparsity weight (0 = disabled)\n",
    "    'label_smoothing': 0.1,  # Label smoothing for classification\n",
    "    'gene_importance_wt': 0,  # Gene importance weight\n",
    "    'gene_importance': 0.25,  # Max gene contribution per bit\n",
    "    'bit_usage_wt': 0,  # Bit usage weight\n",
    "    'bit_usage': 0.1,\n",
    "    'bit_corr_wt': 0,  # Bit correlation weight\n",
    "    'bit_corr': 0.8,\n",
    "    'step_size_wt': 0,  # Step size weight\n",
    "    'step_size_threshold': 1e2,\n",
    "    'step_size_n_steps': 0.1,\n",
    "    'P_scaling': 24,  # Projection scaling\n",
    "    \n",
    "    # Noise parameters (all set to 0 for simplicity, can enable for robustness)\n",
    "    'X_drp_s': 0, 'X_drp_e': 0,\n",
    "    'X_noise_s': 0, 'X_noise_e': 0,\n",
    "    'E_drp_s': 0, 'E_drp_e': 0,\n",
    "    'E_noise_s': 0, 'E_noise_e': 0,\n",
    "    'P_drp_s': 0, 'P_drp_e': 0,\n",
    "    'P_noise_s': 0, 'P_noise_e': 0,\n",
    "    'P_add_s': 4.0, 'P_add_e': 4.0,  # Background signal\n",
    "    'D_drp_s': 0, 'D_drp_e': 0,\n",
    "    'E_perturb_rt': 0,  # Weight perturbation\n",
    "    'E_perb_prct': 0.01,\n",
    "    'E_init_min': 0.001, 'E_init_max': 0.01,\n",
    "    'E_perturb_min': 0.05, 'E_perturb_max': 0.25,\n",
    "    'central_brain': 0,\n",
    "}\n",
    "\n",
    "# Save parameters to CSV file\n",
    "param_file = os.path.join(output_dir, 'parameters.csv')\n",
    "param_df = pd.DataFrame({'values': list(parameters.values())}, index=pd.Index(list(parameters.keys())))\n",
    "param_df.to_csv(param_file)\n",
    "\n",
    "print(f\"Parameters saved to {param_file}\")\n",
    "print(f\"\\nKey parameters:\")\n",
    "print(f\"  - n_bit: {parameters['n_bit']}\")\n",
    "print(f\"  - n_iters: {parameters['n_iters']}\")\n",
    "print(f\"  - n_probes: {parameters['n_probes']}\")\n",
    "print(f\"  - decoder_n_lyr: {parameters['decoder_n_lyr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run CIPHER\n",
    "\n",
    "Initialize and train the CIPHER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CIPHER model\n",
    "print(\"Initializing CIPHER model...\")\n",
    "model = CIPHER(user_parameters_path=param_file)\n",
    "\n",
    "# Initialize the model (loads data, sets up encoder/decoder)\n",
    "if not model.initialize():\n",
    "    print(\"Initialization failed. Check the log file for details.\")\n",
    "    raise RuntimeError(\"Model initialization failed\")\n",
    "\n",
    "print(\"Model initialized successfully!\")\n",
    "print(f\"  - Number of genes: {model.n_genes}\")\n",
    "print(f\"  - Number of bits: {model.I['n_bit']}\")\n",
    "print(f\"  - Number of cell types: {model.n_categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "print(f\"This will run for {parameters['n_iters']} iterations.\")\n",
    "print(f\"Progress will be reported every {parameters['report_rt']} iterations.\")\n",
    "print(f\"Results will be saved to: {model.I['output']}\")\n",
    "\n",
    "model.fit()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"Running evaluation...\")\n",
    "model.evaluate()\n",
    "\n",
    "print(f\"\\nEvaluation complete!\")\n",
    "print(f\"Results saved to: {model.I['output']}\")\n",
    "print(\"\\nKey output files:\")\n",
    "print(\"  - learning_stats.csv: Training statistics\")\n",
    "print(\"  - evaluation_results.csv: Final evaluation metrics\")\n",
    "print(\"  - E_weights.csv: Final encoding weights (probe design)\")\n",
    "print(\"  - model_best.pt: Best model checkpoint\")\n",
    "print(\"  - comprehensive_performance.pdf: Training curves\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
