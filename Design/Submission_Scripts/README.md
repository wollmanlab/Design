# Submission_Scripts Directory

This directory contains shell scripts for submitting computational jobs to a Sun Grid Engine (SGE) cluster. These scripts handle job submission, resource allocation, and environment setup for various computational tasks in the Design project.

## Overview

All scripts in this directory are designed to run on an HPC cluster using SGE. They automatically:
- Detect the current user and set appropriate paths
- Activate the `designer_3.12` conda environment
- Set up proper resource requirements (memory, CPU, runtime)
- Handle job logging and error reporting

## Common Features

All scripts share several common features:

### User Detection
Scripts automatically detect the current user (`rwollman`, `zeh`, or others) and set appropriate paths:
- **Code directory**: Location of the Design repository
- **Runs directory**: Location where run outputs are stored
- **Conda path**: Path to conda initialization script

### Environment Setup
- Loads required modules (conda)
- Activates `designer_3.12` conda environment
- Verifies Python environment before execution

### Resource Requirements
- Default runtime: 24 hours (varies by script)
- Memory: 16-48 GB (varies by script)
- CPU cores: 1-12 (varies by script and job requirements)

## Scripts

### Single Job Submission Scripts

#### `submit_single_job.sh`

Submits a single CIPHER optimization job with a parameter file.

**Usage:**
```bash
qsub submit_single_job.sh /path/to/parameter/file.csv
```

**Features:**
- Runs `CIPHER.py` with the provided parameter file
- Resources: 24h runtime, 16G memory, 1 CPU core
- Outputs logs to `job_logs/job_log.$JOB_ID`

**Example:**
```bash
qsub submit_single_job.sh /path/to/params.csv
```

#### `submit_simulation.sh`

Submits a single simulation job.

**Usage:**
```bash
qsub submit_simulation.sh /path/to/input/file
```

**Features:**
- Runs `simulation.py` with the provided input file
- Resources: 24h runtime, 48G memory, 12 CPU cores
- Outputs logs to `job_logs/job_log.$JOB_ID`

**Example:**
```bash
qsub submit_simulation.sh /path/to/simulation_input.csv
```

#### `sub_python_script.sh`

Generic script to submit any Python script to the cluster.

**Usage:**
```bash
qsub sub_python_script.sh <python_script_name_or_path> [python_script_args...]
```

**Features:**
- Can run any Python script with arbitrary arguments
- Resources: 1h runtime, 64G memory, 1 CPU core
- Includes timing information (`/usr/bin/time -v`)
- Outputs logs to `job_$JOB_NAME.$JOB_ID.out`

**Example:**
```bash
qsub sub_python_script.sh /path/to/script.py arg1 arg2 arg3
```

#### `sub_data_format.sh`

Submits a data formatting job.

**Usage:**
```bash
qsub sub_data_format.sh
```

**Features:**
- Runs `data_format.py` (must be in current directory)
- Resources: 1h runtime, 64G memory, 1 CPU core
- Outputs logs to `job_data_format.$JOB_ID`

**Note:** This script expects `data_format.py` to be in the current working directory.

### Array Job Submission Scripts

#### `sub_multi_param_file_optimization.sh`

Submits an array job to process multiple parameter files in parallel. This is a dual-purpose script that acts as both a submitter and a worker.

**Usage:**
```bash
./sub_multi_param_file_optimization.sh <run_directory_name>
```

**Features:**
- **Submitter mode**: When run directly, creates an array job for all parameter files in `params_files_to_scan/`
- **Worker mode**: When run as part of an array job, processes a specific parameter file
- Automatically reads `n_cpu` from the first parameter file to set resource requirements
- Moves completed files from `params_files_to_scan/` to `params_files_scanned/`
- Adds git version information (commit hash, branch, dirty status) to parameter files
- Creates output directories based on parameter file names
- Resources: 24h runtime, 16G memory, CPU cores from parameter file (default: 1)

**Directory Structure Expected:**
```
<run_directory>/
├── params_files_to_scan/    # Parameter files to process
├── params_files_scanned/    # Completed parameter files (moved here)
├── design_results/          # Output directories for each job
└── job_logs/               # Job log files
```

**Example:**
```bash
./sub_multi_param_file_optimization.sh Run1
```

#### `sub_multi_param_file_optimization_failed_tasks.sh`

Resubmits only the failed tasks from a previous array job run. Similar to `sub_multi_param_file_optimization.sh` but processes only failed tasks.

**Usage:**
```bash
./sub_multi_param_file_optimization_failed_tasks.sh <run_directory_name>
```

**Features:**
- Uses `.failed_tasks_list.txt` instead of `.files_to_process_task_list.txt`
- Automatically generated by `resubmit_failed_tasks.sh`
- Same worker functionality as the main optimization script

**Note:** This script is typically generated automatically by `resubmit_failed_tasks.sh` and should not be called directly.

#### `submit_run_directory.sh`

Submits an array job to process all parameter files found in a run directory's `design_results/` subdirectories.

**Usage:**
```bash
./submit_run_directory.sh <run_directory_name>
```

**Features:**
- Finds all `used_user_parameters.csv` files in `design_results/` subdirectories
- Creates an array job to process each one
- Reads `n_cpu` from parameter files to set resource requirements
- Resources: 24h runtime, 16G memory, CPU cores from parameter file

**Directory Structure Expected:**
```
<run_directory>/
├── design_results/
│   ├── job1/
│   │   └── used_user_parameters.csv
│   ├── job2/
│   │   └── used_user_parameters.csv
│   └── ...
└── job_logs/
```

**Example:**
```bash
./submit_run_directory.sh Run0
```

### Batch Submission Scripts

#### `submit_all_single_jobs_in_run.sh`

Submits individual single jobs (not array jobs) for all parameter files in a run directory.

**Usage:**
```bash
./submit_all_single_jobs_in_run.sh <RunDirName>
```

**Features:**
- Finds all `used_user_parameters.csv` files in `design_results/`
- Submits each as a separate single job using `submit_single_job.sh`
- Useful when you want each job to be independent (not part of an array)

**Example:**
```bash
./submit_all_single_jobs_in_run.sh Run1_Figures
```

### Utility Scripts

#### `resubmit_failed_tasks.sh`

Automatically detects failed tasks from a previous array job and resubmits them.

**Usage:**
```bash
./resubmit_failed_tasks.sh <run_directory_name>
```

**Features:**
- Detects failed tasks by checking job status with `qstat`
- Creates a new task list file (`.failed_tasks_list.txt`) with only failed tasks
- Generates `sub_multi_param_file_optimization_failed_tasks.sh` script
- Deletes failed tasks from the original job
- Submits a new array job for only the failed tasks
- Reads `n_cpu` from parameter files

**Example:**
```bash
./resubmit_failed_tasks.sh Run3
```

**Prerequisites:**
- Job logs must exist in `job_logs/` directory
- Original task list file (`.files_to_process_task_list.txt`) must exist
- Parameter files must still be in `params_files_to_scan/`

## Job Submission Workflow

### Typical Workflow for Parameter File Optimization

1. **Prepare parameter files**: Place all parameter CSV files in `params_files_to_scan/` directory
2. **Submit array job**: 
   ```bash
   ./sub_multi_param_file_optimization.sh Run1
   ```
3. **Monitor jobs**: 
   ```bash
   qstat -u $USER
   ```
4. **Check for failures**: If jobs fail, use:
   ```bash
   ./resubmit_failed_tasks.sh Run1
   ```

### Typical Workflow for Re-running Existing Results

1. **Submit jobs for existing results**:
   ```bash
   ./submit_run_directory.sh Run0
   ```
   or for individual jobs:
   ```bash
   ./submit_all_single_jobs_in_run.sh Run0
   ```

## Directory Structure

### Run Directory Structure

A typical run directory should have the following structure:

```
<run_directory>/
├── params_files_to_scan/          # Parameter files waiting to be processed
├── params_files_scanned/          # Completed parameter files
├── design_results/                # Output directories for each job
│   ├── job1/
│   │   ├── used_user_parameters.csv
│   │   └── ... (other output files)
│   └── job2/
│       └── ...
├── job_logs/                      # SGE job log files
│   ├── job_log.12345.1
│   └── job_log.12345.2
├── .files_to_process_task_list.txt    # List of files for array job
└── .failed_tasks_list.txt             # List of failed tasks (if any)
```

## Resource Requirements Summary

| Script | Runtime | Memory | CPU Cores | Notes |
|--------|---------|--------|-----------|-------|
| `submit_single_job.sh` | 24h | 16G | 1 | Fixed |
| `submit_simulation.sh` | 24h | 48G | 12 | Fixed |
| `sub_python_script.sh` | 1h | 64G | 1 | Fixed |
| `sub_data_format.sh` | 1h | 64G | 1 | Fixed |
| `sub_multi_param_file_optimization.sh` | 24h | 16G | Variable | Reads from param file |
| `submit_run_directory.sh` | 24h | 16G | Variable | Reads from param file |

## Environment Variables

Scripts use the following environment variables (automatically set):
- `CODE_DIR`: Path to Design repository
- `OPT_DIR` / `BASE_DIR`: Path to Runs directory
- `CONDA_PATH`: Path to conda initialization script
- `SGE_TASK_ID`: Task ID when running as part of an array job (set by SGE)

## Git Version Tracking

Array job scripts (`sub_multi_param_file_optimization.sh`, `sub_multi_param_file_optimization_failed_tasks.sh`) automatically add git version information to parameter files:
- `repo_path`: Path to git repository
- `commit_hash`: Full commit hash
- `short_commit_hash`: Short commit hash (7 characters)
- `branch_name`: Current branch name
- `is_dirty`: Whether repository has uncommitted changes

This information is appended to parameter files before execution to track which code version was used.

## Troubleshooting

### Job Fails Immediately
- Check that conda environment `designer_3.12` exists and is accessible
- Verify paths are correct for your user
- Check that parameter files exist and are readable

### Array Job Tasks Fail
- Check individual task logs in `job_logs/` directory
- Verify parameter files are valid CSV format
- Use `resubmit_failed_tasks.sh` to resubmit only failed tasks

### Cannot Find Parameter Files
- Verify directory structure matches expected format
- Check that files are in `params_files_to_scan/` or `design_results/` as appropriate
- Ensure file permissions allow reading

### Resource Issues
- Increase memory or runtime in script headers if jobs are killed
- Adjust `n_cpu` in parameter files if using array jobs
- Check cluster resource limits with `qstat -g c`

## Notes

- All scripts use `qsub` for job submission (SGE/Sun Grid Engine)
- Log files are automatically created in `job_logs/` directories
- Scripts are designed for UCLA Hoffman2 cluster but should work on any SGE cluster with path modifications
- User-specific paths are hardcoded for `rwollman` and `zeh` users; other users will use default path patterns

